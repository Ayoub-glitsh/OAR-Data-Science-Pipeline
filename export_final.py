"""
export_final.py - Export final des donn√©es et rapports
"""
import pandas as pd
import json
import logging
from pathlib import Path
from datetime import datetime
import shutil

def setup_logging():
    """Configure le logging"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )

def create_final_dataset():
    """Cr√©e le jeu de donn√©es final"""
    print("\n" + "="*50)
    print(" CR√âATION DU JEU DE DONN√âES FINAL")
    print("="*50)
    
    output_dir = Path("data/outputs")
    
    # V√©rifie les fichiers n√©cessaires
    required_files = [
        'relational_companies.csv',
        'relational_facilities.csv',
        'relational_links.csv',
        'ai_analysis_simple.csv'
    ]
    
    missing_files = []
    for file in required_files:
        if not (output_dir / file).exists():
            missing_files.append(file)
    
    if missing_files:
        print(f"  Fichiers manquants: {missing_files}")
        print("  Essayer de les r√©g√©n√©rer...")
        return False
    
    # Charge les donn√©es
    companies = pd.read_csv(output_dir / "relational_companies.csv")
    facilities = pd.read_csv(output_dir / "relational_facilities.csv")
    links = pd.read_csv(output_dir / "relational_links.csv")
    ai_results = pd.read_csv(output_dir / "ai_analysis_simple.csv")
    
    print(f" Donn√©es charg√©es:")
    print(f"  ‚Ä¢ Entreprises: {len(companies):,}")
    print(f"  ‚Ä¢ √âtablissements: {len(facilities):,}")
    print(f"  ‚Ä¢ Liens: {len(links):,}")
    print(f"  ‚Ä¢ Analyses IA: {len(ai_results):,}")
    
    # Cr√©e un dossier pour l'export final
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    final_dir = Path("final_export") / f"oar_pipeline_{timestamp}"
    final_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"\n Dossier d'export: {final_dir}")
    
    # 1. Export CSV complet
    print("\n Export CSV...")
    
    companies.to_csv(final_dir / "companies_final.csv", index=False)
    facilities.to_csv(final_dir / "facilities_final.csv", index=False)
    links.to_csv(final_dir / "relationships_final.csv", index=False)
    ai_results.to_csv(final_dir / "ai_analysis_final.csv", index=False)
    
    # 2. Export JSON (pour les applications web)
    print(" Export JSON...")
    
    final_json = {
        "metadata": {
            "export_date": datetime.now().isoformat(),
            "pipeline_version": "1.0",
            "data_source": "Open Apparel Registry",
            "countries_covered": sorted(companies['country'].unique().tolist())
        },
        "summary": {
            "total_companies": int(len(companies)),
            "total_facilities": int(len(facilities)),
            "total_relationships": int(len(links)),
            "sustainable_companies": int(ai_results['has_sustainability'].sum()),
            "sustainability_rate": float((ai_results['has_sustainability'].sum() / len(ai_results)) * 100)
        },
        "companies": companies.head(1000).to_dict(orient='records'),  # Limit√© pour la taille
        "sample_facilities": facilities.head(500).to_dict(orient='records')
    }
    
    with open(final_dir / "data_summary.json", 'w', encoding='utf-8') as f:
        json.dump(final_json, f, indent=2, ensure_ascii=False)
    
    # 3. Export Excel (pour les utilisateurs business)
    print(" Export Excel...")
    
    with pd.ExcelWriter(final_dir / "oar_dataset.xlsx", engine='openpyxl') as writer:
        companies.head(10000).to_excel(writer, sheet_name='Companies', index=False)
        facilities.head(10000).to_excel(writer, sheet_name='Facilities', index=False)
        ai_results.head(10000).to_excel(writer, sheet_name='AI Analysis', index=False)
        
        # Ajoute un r√©sum√©
        summary_df = pd.DataFrame([
            ["Date d'export", datetime.now().strftime("%Y-%m-%d %H:%M")],
            ["Nombre d'entreprises", len(companies)],
            ["Nombre d'√©tablissements", len(facilities)],
            ["Entreprises durables", ai_results['has_sustainability'].sum()],
            ["Taux de durabilit√©", f"{(ai_results['has_sustainability'].sum() / len(ai_results) * 100):.1f}%"],
            ["Pays couverts", len(companies['country'].unique())]
        ], columns=['Metric', 'Value'])
        
        summary_df.to_excel(writer, sheet_name='Summary', index=False)
    
    # 4. Copie les visualisations
    print(" Copie des visualisations...")
    
    viz_dir = final_dir / "visualizations"
    viz_dir.mkdir(exist_ok=True)
    
    image_files = list(output_dir.glob("*.png"))
    for img in image_files:
        shutil.copy2(img, viz_dir / img.name)
    
    # 5. Cr√©e un README pour l'export
    print(" Cr√©ation du README...")
    
    readme_content = f"""# OAR Data Pipeline - Export Final

##  Date d'export
{datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

##  Donn√©es incluses

### Fichiers CSV:
1. `companies_final.csv` - {len(companies):,} entreprises
2. `facilities_final.csv` - {len(facilities):,} √©tablissements
3. `relationships_final.csv` - {len(links):,} relations
4. `ai_analysis_final.csv` - Analyse de durabilit√© IA

### Fichiers structur√©s:
1. `data_summary.json` - R√©sum√© JSON avec m√©tadonn√©es
2. `oar_dataset.xlsx` - Dataset complet Excel

### Visualisations:
{len(image_files)} graphiques dans le dossier `visualizations/`

##  Statistiques cl√©s

- **Entreprises totales**: {len(companies):,}
- **√âtablissements totaux**: {len(facilities):,}
- **Entreprises durables**: {ai_results['has_sustainability'].sum():,}
- **Taux de durabilit√©**: {(ai_results['has_sustainability'].sum() / len(ai_results) * 100):.1f}%
- **Pays couverts**: {len(companies['country'].unique())}

##  Pipeline Information

- **Version**: 1.0
- **Source**: Open Apparel Registry
- **Pays cibles**: Morocco, Spain, Portugal, Italy, France, Greece, Malta

##  Structure des donn√©es

### Table Companies:
- company_id: Identifiant unique
- company_name: Nom nettoy√©
- country: Pays normalis√©
- facility_count: Nombre d'√©tablissements
- record_source: Source des donn√©es

### Table Facilities:
- facility_id: Identifiant unique
- facility_name: Nom de l'√©tablissement
- latitude/longitude: Coordonn√©es GPS
- country/sector/address: Informations suppl√©mentaires

### AI Analysis:
- has_sustainability: D√©tection de durabilit√©
- sustainability_score: Score IA (0-1)
- ai_summary: R√©sum√© automatique

## üìû Contact

Pipeline cr√©√© pour le test technique CommonShare.
Toutes les donn√©es proviennent de l'Open Apparel Registry.
"""
    
    with open(final_dir / "README.md", 'w', encoding='utf-8') as f:
        f.write(readme_content)
    
    print(f"\n Export final cr√©√© dans: {final_dir}")
    
    # Affiche le contenu
    print("\n CONTENU DE L'EXPORT:")
    for item in final_dir.rglob("*"):
        if item.is_file():
            size = item.stat().st_size
            size_str = f"{size:,} bytes" if size < 1024 else f"{size/1024/1024:.1f} MB"
            print(f"  ‚Ä¢ {item.relative_to(final_dir)} ({size_str})")
    
    return final_dir

def generate_pipeline_report():
    """G√©n√®re un rapport du pipeline"""
    print("\n" + "="*50)
    print("üìã RAPPORT DU PIPELINE")
    print("="*50)
    
    report = {
        "timestamp": datetime.now().isoformat(),
        "pipeline_name": "OAR Data Science Pipeline",
        "status": "completed",
        "execution_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "outputs": {}
    }
    
    # Compte les fichiers dans chaque dossier
    for folder in ["data/raw", "data/cleaned", "data/outputs"]:
        folder_path = Path(folder)
        if folder_path.exists():
            files = list(folder_path.glob("*"))
            report["outputs"][folder] = {
                "file_count": len(files),
                "files": [f.name for f in files[:10]]  # Premier 10 seulement
            }
    
    # Calcule les statistiques finales
    try:
        companies_path = Path("data/outputs") / "relational_companies.csv"
        if companies_path.exists():
            companies = pd.read_csv(companies_path)
            report["statistics"] = {
                "total_companies": int(len(companies)),
                "countries": int(companies['country'].nunique()),
                "avg_facilities_per_company": float(companies.get('facility_count', 1).mean())
            }
    except:
        pass
    
    # Sauvegarde le rapport
    report_path = Path("logs") / f"pipeline_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(report_path, 'w') as f:
        json.dump(report, f, indent=2)
    
    print(f" Rapport sauvegard√©: {report_path}")
    
    # Affiche un r√©sum√©
    print("\ R√âSUM√â DU PIPELINE:")
    if "statistics" in report:
        stats = report["statistics"]
        print(f"  ‚Ä¢ Entreprises: {stats.get('total_companies', 0):,}")
        print(f"  ‚Ä¢ Pays: {stats.get('countries', 0)}")
        print(f"  ‚Ä¢ √âtablissements/entreprise: {stats.get('avg_facilities_per_company', 0):.2f}")
    
    print(f"  ‚Ä¢ Fichiers bruts: {report['outputs'].get('data/raw', {}).get('file_count', 0)}")
    print(f"  ‚Ä¢ Fichiers nettoy√©s: {report['outputs'].get('data/cleaned', {}).get('file_count', 0)}")
    print(f"  ‚Ä¢ Fichiers de sortie: {report['outputs'].get('data/outputs', {}).get('file_count', 0)}")
    
    return report_path

def main():
    """Fonction principale"""
    setup_logging()
    
    print("\n" + "="*60)
    print(" EXPORT FINAL DU PIPELINE")
    print("="*60)
    
    try:
        # 1. Cr√©er le dataset final
        export_dir = create_final_dataset()
        
        if not export_dir:
            print(" √âchec de la cr√©ation du dataset final")
            return False
        
        # 2. G√©n√©rer le rapport
        report_path = generate_pipeline_report()
        
        print("\n" + "="*60)
        print(" EXPORT TERMIN√â AVEC SUCC√àS!")
        print("="*60)
        print(f"\n Toutes les donn√©es sont disponibles dans:")
        print(f"   ‚Ä¢ {export_dir}")
        print(f"   ‚Ä¢ {report_path}")
        print("\n Le pipeline est complet et pr√™t √† √™tre soumis!")
        
        return True
        
    except Exception as e:
        print(f"\n ERREUR: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    main()